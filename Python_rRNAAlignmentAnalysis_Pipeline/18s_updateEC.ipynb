{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c84646-ddb1-461f-ae13-1335250f511e",
   "metadata": {},
   "source": [
    "# Pipeline3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a2d3c5-2ba3-4c55-b8a2-d0d0a786b0dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import igraph as ig\n",
    "import matplotlib.pyplot as plt\n",
    "from collections.abc import Iterable\n",
    "import ete3\n",
    "from collections import (defaultdict,OrderedDict,Counter)\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1113e9c-1409-4e94-83d3-1278c6e06cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        input_file = '/data/judong/Python_rRNA_alingmentMergePipeline/InputUpdatedBash/BashInput/RNA_Genes/HG001_18S_clusters/Combined/HG001_18S_cluster23.fa'\n",
    "        records = SeqIO.to_dict(SeqIO.parse(input_file, \"fasta\"))\n",
    "        ids = list(SeqIO.to_dict(SeqIO.parse(input_file, \"fasta\")).keys())\n",
    "        len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8359da2-3f41-4171-a338-3f06cfb956ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0061c02-f277-43ff-a490-4917c1a28414",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateMatrix():\n",
    "        AcChrList = ['chr13','chr14','chr15','chr21','chr22']\n",
    "        input_file = '/data/judong/Python_rRNA_alingmentMergePipeline/InputUpdatedBash/BashInput/RNA_Genes/HG001_18S_clusters/Combined/HG001_18S_cluster23.fa'\n",
    "        records = SeqIO.to_dict(SeqIO.parse(input_file, \"fasta\"))\n",
    "        ids = list(SeqIO.to_dict(SeqIO.parse(input_file, \"fasta\")).keys())\n",
    "        ids = list(\n",
    "                    ids[i]\n",
    "            for i in range(0,len(ids))\n",
    "                if ids[i][0:5] in AcChrList\n",
    "        )\n",
    "        array =[\n",
    "            y\n",
    "            for i in range(0,len(ids))\n",
    "                if ids[i][0:5] in AcChrList\n",
    "                    for y in records[ids[i]].seq\n",
    "                    \n",
    "]\n",
    "        m = np.array(array)\n",
    "        M = np.reshape(m, (-1, len(records[ids[1]].seq)))\n",
    "        M_rows, M_cols = M.shape \n",
    "        return M,M_rows,M_cols,ids\n",
    "def CreateReadingNameArray(ids):\n",
    "    ReadingNameArray = np.array([ids[i] for i in range(0,len(ids))])\n",
    "    return ReadingNameArray\n",
    "def RemoveTerminalGaps(M,M_rows, M_cols):\n",
    "\n",
    "    ##find all gaps\n",
    "    gapRowCoords,gapColCoords = np.where(M == '-')\n",
    "    gapCoords=np.vstack((gapRowCoords,gapColCoords)).T\n",
    "\n",
    "\n",
    "    #the rows with 1st 3 chars are gaps\n",
    "    MissInfoRowCoordsF=[\n",
    "        (i)\n",
    "    for i,j in gapCoords\n",
    "        if j==0 and M[i,(j+1)]=='-' and M[i,(j+2)]=='-'\n",
    "]    \n",
    "\n",
    "    #the rows with last 3 chars are gaps\n",
    "    MissInfoRowCoordsB=[\n",
    "        (i)\n",
    "        for i,j in gapCoords\n",
    "            if j==M_cols-1 and M[i,(j-2)]=='-'and M[i,(j-3)]=='-'\n",
    "] \n",
    "\n",
    "    #conbine Fw and Bw\n",
    "    MissInfoRowCoords=list(set(MissInfoRowCoordsF + MissInfoRowCoordsB))\n",
    "        # mask rows with terminal gaps \n",
    "    for i in MissInfoRowCoordsF:\n",
    "        for j in range(0,M_cols):\n",
    "            if M[i,j]== '-':\n",
    "                M[i,j] = '+'\n",
    "            if M[i,j+1]!= '-':\n",
    "                break\n",
    "    for i in MissInfoRowCoordsB:\n",
    "        for j in reversed(range(0,M_cols)):\n",
    "            if M[i,j]== '-':\n",
    "                M[i,j] = '+'\n",
    "            if M[i,j-1]!= '-':\n",
    "                break\n",
    "   \n",
    "    M_rows,M_cols = M.shape\n",
    "    print(len(MissInfoRowCoords),\" rows reads with terminal gaps have been Masked.\")\n",
    "    return M,M_rows,M_cols,MissInfoRowCoords\n",
    "def CreateReadingNameDicts(ReorderRN):\n",
    "    ReadingDicts = {}\n",
    "    for i in range(0,len(ids)):\n",
    "        ReadingDicts[i] = [ReorderRN[i]]\n",
    "    return ReadingDicts\n",
    "def CreateReadingNameDicts(ReorderRN):\n",
    "    ReadingDicts = {}\n",
    "    for i in range(0,len(ids)):\n",
    "        ReadingDicts[i] = [ReorderRN[i]]\n",
    "    \n",
    "    return ReadingDicts\n",
    "def ErrorMasking(M,M_rows,M_cols,T):\n",
    "\n",
    "    for x in range(0,M_cols):\n",
    "        ele_array = np.unique(M[:,x])\n",
    "    \n",
    "        if len(ele_array) == 1:\n",
    "                pass\n",
    "        else:\n",
    "               for i in ele_array:\n",
    "                ele_per = ((M[:,x]==i).sum(axis=0))/M_rows\n",
    "           \n",
    "                if ele_per > T:\n",
    "                    pass\n",
    "                elif ele_per <= T and ((M[:,x]=='-').sum(axis=0))>(M_rows*(9/10)):\n",
    "                    for j in (np.where(M[:,x]==i)):\n",
    "                        M[j,x]='+'\n",
    "                else:\n",
    "                    for j in (np.where(M[:,x]==i)):\n",
    "                        M[j,x]='n'\n",
    "def StringMatrixToNumber (M1):\n",
    "        \n",
    "        M2=M1\n",
    "\n",
    "        M2[M2 == 'A']=int(\"1\")\n",
    "        M2[M2 == 'C']=int(\"2\")\n",
    "        M2[M2 == 'T']=int(\"3\")\n",
    "        M2[M2 == 'G']=int(\"4\")\n",
    "        M2[M2 == 'n']=int(\"0\")\n",
    "        M2[M2 == '-']=int(\"5\")#-2\n",
    "        M2[M2 == '+']=int(\"6\")#-1\n",
    "        Int = np.vectorize(lambda x: int(x))\n",
    "        M2 = Int(M1)   \n",
    "        M2[M2 == 5]=-2\n",
    "        M2[M2 == 6]=-1\n",
    "        print(\"First three position of number Matrix:\",M2[0,0],M2[0,1],M2[0,2],\"Datatype:\",M2.dtype)\n",
    "        return M2\n",
    "def CalculatePairWiseDistance_sk(u,v):\n",
    "    a = np.bitwise_and(np.bitwise_and(u > 0,v > 0),(u!=v)).sum(axis=0)\n",
    "    b = np.bitwise_and(u == -2,v>=0).sum(axis=0) + np.bitwise_and(v == -2,u>=0).sum(axis=0)\n",
    "    return a+b\n",
    "def CalculatePairWiseDistance(M2,ReadingD):\n",
    "        M1=M2\n",
    "        # distance matrix with M_rows* M_rows shape\n",
    "        Mdis0 = np.full((M_rows,M_rows),-1)\n",
    "        repeat = np.full(M_rows,1,dtype=bool)\n",
    "        counter = np.full(M_rows,1,dtype=int)\n",
    "        nmcounter = 0\n",
    "        #1.1 If we want to see How Merge happens unhash the code below\n",
    "        #MergeDicts_Pd = pd.DataFrame( {'SeqID_i':[],  'SeqID_j':[]})\n",
    "        for i in range(0,M_rows-1):\n",
    "            if repeat[i]==False:\n",
    "                    continue\n",
    "            else:\n",
    "                for j in range(i+1,M_rows):\n",
    "                    if repeat[j]==False:\n",
    "                         continue\n",
    "                    else:\n",
    "                        a = np.bitwise_and(np.bitwise_and(M1[i,]>0,M1[j,]>0),(M1[i,]!=M1[j,])).sum(axis=0)\n",
    "                        b = np.bitwise_and(M1[i,]== -2,M1[j,]>=0).sum(axis=0) + np.bitwise_and(M1[j,]== -2,M1[i,]>=0).sum(axis=0)\n",
    "                        Mdis0[i,j]= a+b  \n",
    "                        if a+b == 0:\n",
    "                            counter[i]+=1\n",
    "                            counter[j]-=1\n",
    "                            repeat[j]= False\n",
    "                            for ele in ReadingD[j]:\n",
    "                                if ele not in ReadingD[i]:\n",
    "                                    ReadingD[i].append(ele)\n",
    "                            #1.1 If we want to see How Merge happens unhash the code below\n",
    "                            #df2 = pd.DataFrame([[i,j]], columns=['SeqID_i','SeqID_j'])\n",
    "                            #MergeDicts_Pd = pd.concat([df2, MergeDicts_Pd])\n",
    "                      #  else:\n",
    "        return counter,Mdis0,ReadingD#MergeDicts_Pd\n",
    "def n_correction(Mdis0,M1):    \n",
    "    M_Rows,M_Cols = M1.shape\n",
    "    def unique_rows(a):\n",
    "        a = np.ascontiguousarray(a)\n",
    "        unique_a = np.unique(a.view([('', a.dtype)]*a.shape[1]))\n",
    "        return unique_a.view(a.dtype).reshape((unique_a.shape[0], a.shape[1]))\n",
    "    \n",
    "    r,c = np.where(Mdis0 == 0)\n",
    "    XCoords=np.vstack((r,c)).T\n",
    "    XCoords = unique_rows(np.sort(XCoords))\n",
    "    for (x,y) in XCoords:\n",
    "        for i in range(0,M_Cols-1):\n",
    "            if M1[x,i]==0 or M1[x,i]==-1: \n",
    "                M1[x,i] = M1[y,i]\n",
    "    return M1\n",
    "def reduction_Matrix(M2,counter):\n",
    "    M_rows,M_cols = M2.shape\n",
    "    dict1={}\n",
    "    for i,j in enumerate(counter):\n",
    "        dict1[i]=j\n",
    "    array2 = np.zeros((len(dict1.keys()),M_cols))\n",
    "    for i in range(0,len(dict1.keys())):\n",
    "        if dict1[i]==0:\n",
    "            continue\n",
    "        else:\n",
    "            array2[i]=M2[list(dict1.keys())[i]]\n",
    "    return array2\n",
    "def ReducedReadingD(counter,ReadingD):\n",
    "    ReadingD2 = {}\n",
    "    a = np.array([\n",
    "        x\n",
    "    for x,y in enumerate(counter)\n",
    "        if y != 0\n",
    "       \n",
    "    ])\n",
    "    for i in a:\n",
    "        ReadingD2[i]= ReadingD[i]\n",
    "    return ReadingD2\n",
    "def ReindexAfterReduction(ReadingD2):\n",
    "    Reading_Pd = {}\n",
    "    for x,y in enumerate(ReadingD2.keys()):\n",
    "        Reading_Pd[x]=ReadingD2[y]\n",
    "    return Reading_Pd\n",
    "def Export(Mdis):\n",
    "    from numpy import savetxt\n",
    "    savetxt('Dist_Matrix_v2.0_reduction_1e2.csv',Mdis,fmt='%i',delimiter=',')\n",
    "    #savetxt('Counter1e2.csv',counter,fmt='%i',delimiter=',')\n",
    "    #savetxt('Merge_list.csv',Merge_list,fmt='%i',delimiter=',')#   continue\n",
    "def NumberMatrixToString (reduced_array):\n",
    "        rows,cols = reducedarray.shape\n",
    "       \n",
    "        array_rev = np.full((rows,cols),'a')\n",
    "        for i in range(0,rows):\n",
    "            for j in range(0,cols):\n",
    "                if reducedarray[i,j]==1.0:\n",
    "                    array_rev[i,j]='A'\n",
    "                elif reducedarray[i,j]==2.0:\n",
    "                    array_rev[i,j]='C'\n",
    "                elif reducedarray[i,j]==3.0:\n",
    "                    array_rev[i,j]='T'\n",
    "                elif reducedarray[i,j]==4.0:\n",
    "                    array_rev[i,j]='G'\n",
    "                elif reducedarray[i,j]==0.0:\n",
    "                    array_rev[i,j]='n'\n",
    "                else:\n",
    "                    array_rev[i,j]= '-'\n",
    "        return array_rev\n",
    "def StringMatrixToSeq(array_rev):\n",
    "    ### input:string matrix in ['a','c','t','g'], output is ['actg'] array\n",
    "    rows,cols = array_rev.shape\n",
    "    separator = ''\n",
    "    seq_array = np.array([\n",
    "    separator.join(array_rev[i,])\n",
    "        for i in range(0,rows)\n",
    "            ])\n",
    "    return seq_array\n",
    "def TerminalGapAndNCorrection(array_rev,Counter_Pd):\n",
    "    seq_rows, seq_cols = array_rev.shape\n",
    "    array_rev_GapMarked,seq_rows,seq_cols,MissInfoRowCoords = RemoveTerminalGaps(array_rev,seq_rows, seq_cols)\n",
    "    most_commontype = Counter_Pd['C_Num'].idxmax()\n",
    "    for i in range(0,len(array_rev_GapMarked)):\n",
    "        for j in range(0,seq_cols):\n",
    "            if array_rev_GapMarked[i][j] == '+' or array_rev_GapMarked[i][j] == 'n':\n",
    "                array_rev_GapMarked[i][j] = array_rev_GapMarked[12][j]\n",
    "    array_rev =   array_rev_GapMarked\n",
    "    return array_rev\n",
    "def RemoveGap(array_rev):\n",
    "    a,b = array_rev.shape\n",
    "    array_gap = [\n",
    "        i \n",
    "        for i in range(0,b-1)\n",
    "            if sum(array_rev[:,i] == '-')== a\n",
    "    ]\n",
    "    array_rev = np.delete(array_rev,array_gap,1)\n",
    "    return array_rev\n",
    "def FastaRecords(seq_array):\n",
    "    rows, = seq_array.shape\n",
    "    separator = ''\n",
    "    a = np.array([seq_array[i].item() for i in range(0,rows)])\n",
    "    b = [Seq(i) for i in a]\n",
    "    records = [\n",
    "           SeqRecord(Seq(seq), id = str(index), description = \"\") \n",
    "           for index,seq in enumerate(b) \n",
    "    ]\n",
    "    SeqIO.write(records, 'Merged_1e2' ,\"fasta\")\n",
    "    return \n",
    "def GenerateCounterDataFrame(counter):\n",
    "    ### Input: Counter that of copyNumbers\n",
    "    ### NonZeroArray: Counter without 0 copyNumbers\n",
    "    ### Counter_Pd: DataFrame with Columns: CopyNumber:C_Num and SeqIDs    \n",
    "    Counnter_Dict = {}\n",
    "    NonZeroDict = {}\n",
    "    \n",
    "    for i,j in enumerate(counter):\n",
    "        Counnter_Dict[i] = j\n",
    "    \n",
    "    for i in range(0,len(Counnter_Dict.values())):\n",
    "        if counter [i]!=0:\n",
    "            NonZeroDict[i] = Counnter_Dict[i]\n",
    "    \n",
    "    NonZeroArray = np.array(list(NonZeroDict.values()))\n",
    "    \n",
    "    Counter_Pd = pd.DataFrame(NonZeroArray)\n",
    "    Counter_Pd = Counter_Pd.rename(columns = { 0:'C_Num'})\n",
    "    Counter_Pd['SeqID'] = range(0,len(Counter_Pd))\n",
    "    Counter_Pd.to_csv('Counter_Pd.csv',sep=',')\n",
    "    return Counter_Pd\n",
    "def DeleteIsolateSingleReadings(Mdis,Counter_Pd,T):\n",
    "    str = f\"MdisM{T}\" \n",
    "    exec(\"%s = Mdis<T \" % (str))\n",
    "    DistList, = np.where(eval(str).sum(axis=0)==1)\n",
    "    DistList_CNumblowT = np.array([\n",
    "        i\n",
    "    for i in DistList\n",
    "        if int(Counter_Pd.loc[Counter_Pd[\"SeqID\"]==i,\"C_Num\"]) == 1\n",
    "]) \n",
    "    Counter_Pd_removeDist = Counter_Pd.drop(DistList_CNumblowT)\n",
    "  \n",
    "    return Counter_Pd_removeDist,DistList_CNumblowT\n",
    "def CoordsOfOnedistInDistMatrix(Mdis):\n",
    "    \n",
    "    def unique_rows(a):\n",
    "        a = np.ascontiguousarray(a)\n",
    "        unique_a = np.unique(a.view([('', a.dtype)]*a.shape[1]))\n",
    "        return unique_a.view(a.dtype).reshape((unique_a.shape[0], a.shape[1]))\n",
    "    \n",
    "    r,c = np.where(Mdis == 1)\n",
    "    XCoords=np.vstack((r,c)).T\n",
    "    XCoords = unique_rows(np.sort(XCoords))\n",
    "    OneDist_pd = pd.DataFrame(XCoords).rename(columns = { 0:'Seq_ID_i',1:'Seq_ID_j'})\n",
    "    return OneDist_pd\n",
    "def FindUniqueOneDistReading(OneDist_pd,Counter_Pd,ReadingD2):\n",
    "    droplist2 = {}\n",
    "    for i in range(0,len(Counter_Pd)):\n",
    "        for j in OneDist_pd.loc[OneDist_pd[\"Seq_ID_i\"]==i,\"Seq_ID_j\"]:\n",
    "            if len(OneDist_pd.loc[OneDist_pd[\"Seq_ID_j\"]==j,\"Seq_ID_i\"]) == 1:\n",
    "                for ele in ReadingD2[j]:\n",
    "                    if ele not in ReadingD2[i]:\n",
    "                        ReadingD2[i].append(ele)\n",
    "                droplist2[j]= j\n",
    "                Counter_Pd.loc[Counter_Pd['SeqID'] == i,['C_Num']] += int(Counter_Pd.loc[Counter_Pd['SeqID']==j,'C_Num'])\n",
    "                droplist2_array = np.array(list(droplist2.keys()))\n",
    "    try:\n",
    "        return droplist2_array,Counter_Pd,ReadingD2\n",
    "    except:\n",
    "        return [],Counter_Pd,ReadingD2\n",
    "def Dropthereadings(Counter_Pd,droplistT1_array):\n",
    "    Counter_Pd = Counter_Pd.drop(droplistT1_array,axis=0)\n",
    "    for i in droplistT1_array:\n",
    "        Reading_Pd.pop(i)\n",
    "    return Counter_Pd,Reading_Pd       \n",
    "def OneCopyReadingList(Counter_Pd):\n",
    "    OneCopyDf = Counter_Pd[Counter_Pd[\"C_Num\"]== 1]\n",
    "    OneCopyDfSeqID = np.array(list(OneCopyDf[\"SeqID\"]))\n",
    "    return OneCopyDf,OneCopyDfSeqID\n",
    "def DefineUnCertain(Counter_Pd,T1,T2):\n",
    "    #Input: T1: lower bond of UnCertainreadings, T2: upperBond of UnCertainReadings\n",
    "    UnCertainlist = (Counter_Pd[Counter_Pd[\"C_Num\"] >T1]).loc[(\n",
    "        Counter_Pd[Counter_Pd[\"C_Num\"] >T1])['C_Num']<T2]\n",
    "    UnCertainID = np.array(list(UnCertainlist['SeqID']))\n",
    "    return UnCertainlist,UnCertainID\n",
    "\n",
    "def DefineCertain(Counter_Pd,T1):\n",
    "    #Input: T1 define the lower limit for certain C_Num\n",
    "    Certainlist = Counter_Pd[Counter_Pd[\"C_Num\"] >=T1] \n",
    "    CertainID = np.array(list(Certainlist['SeqID']))\n",
    "    return Certainlist,CertainID\n",
    "def Split_Merge_1copy_to_UnCertain(UnCertainlist,UnCertainID,OneCopyDfSeqID,Mdis,Counter_Pd,Reading_Pd,T):\n",
    "    Mdis_Pd = pd.DataFrame(Mdis)\n",
    "    droplist_PartialUnCertain = {}\n",
    "    \n",
    "    for i in UnCertainID:\n",
    "        for j in list((Mdis_Pd.loc[Mdis_Pd[i]==T]).index):\n",
    "            if j in OneCopyDfSeqID:\n",
    "                droplist_PartialUnCertain[j] = j\n",
    "                droplist_PartialUnCertain_array = np.array(list(droplist_PartialUnCertain.keys()))\n",
    "                UnCertainlist.loc[UnCertainlist['SeqID'] == i,'C_Num'] +=  float(UnCertainlist.loc[UnCertainlist['SeqID'] == i,'C_Num']) /sum([\n",
    "                float(Counter_Pd.loc[Counter_Pd['SeqID']==k,'C_Num'])\n",
    "                for k in list(Mdis_Pd.loc[Mdis_Pd[j]==T].index)\n",
    "                ])\n",
    "                for ele in Reading_Pd[j]:\n",
    "                    if ele not in Reading_Pd[i]:\n",
    "                           Reading_Pd[i].append(ele)   \n",
    "    try:    \n",
    "        return UnCertainlist,droplist_PartialUnCertain_array,Reading_Pd\n",
    "    except:\n",
    "        return UnCertainlist,[],Reading_Pd\n",
    "def Split_Merge_1copy_to_certain(Certainlist,CertainID,OneCopyDfSeqID,Mdis,Counter_Pd,Reading_Pd,T):\n",
    "    droplist_PartialCertain = {}\n",
    "    Mdis_Pd = pd.DataFrame(Mdis) \n",
    "    for i in CertainID:\n",
    "        for j in list((Mdis_Pd.loc[Mdis_Pd[i]==T]).index):\n",
    "            if j in OneCopyDfSeqID:\n",
    "                droplist_PartialCertain[j] = j\n",
    "                droplist_PartialCertain_array = np.array(list(droplist_PartialCertain.keys()))\n",
    "                Certainlist.loc[Certainlist['SeqID'] == i,'C_Num'] += float(Certainlist.loc[Certainlist['SeqID'] == i,'C_Num']) /sum([\n",
    "                float(Counter_Pd.loc[Counter_Pd['SeqID']==k,'C_Num'])\n",
    "                for k in list(Mdis_Pd.loc[Mdis_Pd[j]==T].index)\n",
    "                ])\n",
    "                for ele in Reading_Pd[j]:\n",
    "                    if ele not in Reading_Pd[i]:\n",
    "                        Reading_Pd[i].append(ele)                \n",
    "    try: \n",
    "        return Certainlist,droplist_PartialCertain_array,Reading_Pd\n",
    "    except:\n",
    "        return Certainlist,[],Reading_Pd\n",
    "\n",
    "\n",
    "def OneCopyreadingNotJoinedMerging(OneCopyDf,OneCopyDfSeqID,droplist_PartialUnCertain_array, droplist_PartialCertain_array):\n",
    "    a = np.concatenate((droplist_PartialUnCertain_array, droplist_PartialCertain_array), axis=None)\n",
    "    ReadingsInvolved = np.unique(a)\n",
    "    OneCopyNotInvolved_array = OneCopyDfSeqID[~np.isin(OneCopyDfSeqID, ReadingsInvolved)]\n",
    "    OneCopyNotInvolved_df = OneCopyDf.drop(ReadingsInvolved)\n",
    "    return OneCopyNotInvolved_df,OneCopyNotInvolved_array\n",
    "def CoordsOfTdistInDistMatrix(Mdis,T,arrayofInterest):\n",
    "    \n",
    "    def unique_rows(a):\n",
    "        a = np.ascontiguousarray(a)\n",
    "        unique_a = np.unique(a.view([('', a.dtype)]*a.shape[1]))\n",
    "        return unique_a.view(a.dtype).reshape((unique_a.shape[0], a.shape[1]))\n",
    "    \n",
    "    r,c = np.where(Mdis == T)\n",
    "    r1 = np.array([\n",
    "        i\n",
    "        for i in r\n",
    "            if i in arrayofInterest \n",
    "    ])\n",
    "    c1 = np.array([\n",
    "        i\n",
    "        for i in c\n",
    "            if i in arrayofInterest \n",
    "    ])\n",
    "    XCoords=np.vstack((r1,c1)).T\n",
    "    XCoords = unique_rows(np.sort(XCoords))\n",
    "    TDist_pd = pd.DataFrame(XCoords).rename(columns = { 0:'Seq_ID_i',1:'Seq_ID_j'})\n",
    "    return TDist_pd\n",
    "def autodistmerge(Mdis,arrayofInterest,OneCopyNotInvolved_df,OneCopyNotInvolved_array,Reading_Pd,ids,Counter_Pd):\n",
    "    \n",
    "    T = 1\n",
    "    CheckPoint = 1\n",
    "    while CheckPoint > 0.1:\n",
    "        T +=1\n",
    "        TDist_pd = CoordsOfTdistInDistMatrix(Mdis,T,arrayofInterest)\n",
    "        droplist_Unique_array,Counter_Pd,Reading_Pd = FindUniqueTDistReading(TDist_pd,Counter_Pd,OneCopyNotInvolved_array,Reading_Pd)\n",
    "        UnCertainlist,UnCertainID = DefineUnCertain(Counter_Pd,5,10)\n",
    "        Certainlist,CertainID = DefineCertain(Counter_Pd,10)\n",
    "        UnCertainlist,droplist_PartialUnCertain_array,Reading_Pd = Split_Merge_1copy_to_UnCertain(UnCertainlist,UnCertainID,OneCopyNotInvolved_array,Mdis,Counter_Pd,Reading_Pd,T)\n",
    "        Certainlist,droplist_PartialCertain_array,Reading_Pd= Split_Merge_1copy_to_certain(Certainlist,CertainID,OneCopyNotInvolved_array,Mdis,Counter_Pd,Reading_Pd,T)\n",
    "        OneCopyNotInvolved_df,OneCopyNotInvolved_array =  OneCopyreadingNotJoinedMerging(OneCopyNotInvolved_df,OneCopyNotInvolved_array,droplist_PartialUnCertain_array, droplist_PartialCertain_array)\n",
    "        arrayofInterest = np.concatenate([CertainID, UnCertainID, OneCopyNotInvolved_array])\n",
    "        CheckPoint = len(OneCopyNotInvolved_array)/len(ids) \n",
    "    else:\n",
    "        print(len(OneCopyNotInvolved_array),'of One Copy Readinds has not involved in merging,','Merging happened up to',T,'dist')\n",
    "def FindUniqueTDistReading(T_pd,Counter_Pd,OneCopyNotInvolved_array,Reading_Pd):\n",
    "    droplist_TUnique = {}\n",
    "    for i in range(0,len(Counter_Pd)):\n",
    "        for j in T_pd.loc[T_pd[\"Seq_ID_i\"]==i,\"Seq_ID_j\"]:\n",
    "            if len(T_pd.loc[T_pd[\"Seq_ID_i\"]==j,\"Seq_ID_j\"]) == 1:\n",
    "                if j in OneCopyNotInvolved_array:\n",
    "                    droplist_TUnique[j]= j\n",
    "                    Counter_Pd.loc[Counter_Pd['SeqID'] == i,['C_Num']] += float(Counter_Pd.loc[Counter_Pd['SeqID']==j,'C_Num'])\n",
    "                    droplist_TUnique_array = np.array(list(droplist_TUnique.keys()))\n",
    "                    for ele in Reading_Pd[j]:\n",
    "                        if ele not in Reading_Pd[i]:\n",
    "                            Reading_Pd[i].append(ele)                \n",
    "    try:\n",
    "        return droplist_TUnique_array,Counter_Pd,Reading_Pd\n",
    "    except:\n",
    "        return [],Counter_Pd,Reading_Pd\n",
    "def CalChrCount(ids,Df):    \n",
    "\n",
    "    UniChrPos = np.array( ['chr13','chr14','chr15','chr21','chr22'])\n",
    "    IDarray = np.array(Df['SeqID'])\n",
    "    data = np.full((len(IDarray),len(UniChrPos)),0)\n",
    "    UniChrPos_df = pd.DataFrame(data,columns=UniChrPos)\n",
    "    UniChrPos_df['SeqID']=IDarray\n",
    "    for key in Reading_Pd.keys():\n",
    "        for x,y in enumerate(Reading_Pd[key]):\n",
    "            try:\n",
    "                UniChrPos_df.loc[UniChrPos_df['SeqID']==key,[Reading_Pd[key][x][0:5]]] +=1\n",
    "            except:\n",
    "                continue\n",
    "    IDlist = list(UniChrPos_df['SeqID'])\n",
    "    UniChrPos_df = UniChrPos_df.set_index('SeqID',drop=True)   \n",
    "    return IDlist,UniChrPos_df\n",
    "def ExpChrCount(CU_count,C_count):\n",
    "    columns_CU = list(C_count.columns)\n",
    "    columns_C =  list(C_count.columns)\n",
    "    data_CU = np.full((1,len(columns_CU)),0)\n",
    "    data_C = np.full((1,len(columns_C)),0)\n",
    "    EmptyRef_CU = pd.DataFrame(data_CU,columns=columns_CU,index=[111111])\n",
    "    EmptyRef_C = pd.DataFrame(data_C,columns=columns_C,index=[111111])\n",
    "    CertainAndUnEx = CU_count.append(EmptyRef_CU)\n",
    "    CertainEx = C_count.append(EmptyRef_C)\n",
    "    CertainAndUnEx.to_csv(F\"LociPosof{len(CertainAndUnEx)}Readings.csv\",sep = ',')\n",
    "    CertainEx.to_csv(F\"LociPosof{len(CertainEx)}Readings.csv\",sep = ',')\n",
    "def FastaRecords_Final_CandUnC(MergedSeqRecords_array,IDlist,S):\n",
    "    rows, = MergedSeqRecords_array.shape\n",
    "    input_file = '/data/judong/Bash_RibosomalRnaGeneAlignment/AlignmentResults/Ref/ref.fas'\n",
    "    records = SeqIO.to_dict(SeqIO.parse(input_file, \"fasta\"))\n",
    "    ids = list(SeqIO.to_dict(SeqIO.parse(input_file, \"fasta\")).keys())\n",
    "    array =[\n",
    "            y\n",
    "            for i in range(0,len(ids))\n",
    "                for y in records[ids[i]].seq\n",
    "        ]\n",
    "    rows, = MergedSeqRecords_array.shape\n",
    "    separator = ''\n",
    "    a = np.array([MergedSeqRecords_array[i].item() for i in range(0,rows)])\n",
    "    b = [Seq(i) for i in a]\n",
    "    c = np.array((IDlist,b),dtype=object).T\n",
    "    d = np.array([['ref',(records[ids[0]].seq)]],dtype=object)\n",
    "    e = np.array([['ref',(records[ids[1]].seq)]],dtype=object)\n",
    "    if S == 18:\n",
    "        f = np.append(c,d,axis = 0)\n",
    "    if S == 28:\n",
    "        f = np.append(c,e,axis = 0)\n",
    "\n",
    "    \n",
    "    records2 = [\n",
    "           SeqRecord(Seq(seq), id = str(index), description = \"\") \n",
    "           for index,seq in f \n",
    "    ]\n",
    "    \n",
    "    SeqIO.write(records2, 'Final_Merged_CertainandUnCertain' ,\"fasta\")\n",
    "    return \n",
    "def CertainC_Numlist(Certain_Df,CertainIDlist):\n",
    "    CertainC_Numlist = np.array([\n",
    "        Certain_Df['C_Num'][i]\n",
    "             for i in CertainIDlist   \n",
    "    ])\n",
    "    CertainID_CNum = np.array([\n",
    "    f\"ID{CertainIDlist[i]}_C{int(CertainC_Numlist[i])}\"\n",
    "        for i in range(0,len(CertainIDlist))\n",
    "    ])\n",
    "    return CertainID_CNum\n",
    "def FastaRecords_Final_Certain(Certain,CertainID_CNum,S):\n",
    "    input_file = '/data/judong/Bash_RibosomalRnaGeneAlignment/AlignmentResults/Ref/ref.fas'\n",
    "    records = SeqIO.to_dict(SeqIO.parse(input_file, \"fasta\"))\n",
    "    ids = list(SeqIO.to_dict(SeqIO.parse(input_file, \"fasta\")).keys())\n",
    "    array =[\n",
    "            y\n",
    "            for i in range(0,len(ids))\n",
    "                for y in records[ids[i]].seq\n",
    "        ]\n",
    "    rows, = Certain.shape\n",
    "    separator = ''\n",
    "    a = np.array([Certain[i].item() for i in range(0,rows)])\n",
    "    b = [Seq(i) for i in a]\n",
    "    c = np.array((CertainID_CNum,b),dtype=object).T\n",
    "    d = np.array([['ref',(records[ids[0]].seq)]],dtype=object)\n",
    "    e = np.array([['ref',(records[ids[1]].seq)]],dtype=object)\n",
    "    if S == 18:\n",
    "        d = np.append(c,d,axis = 0)\n",
    "    if S == 28:\n",
    "        d = np.append(c,e,axis = 0)\n",
    "\n",
    "    \n",
    "    records2 = [\n",
    "           SeqRecord(Seq(seq), id = str(index), description = \"\") \n",
    "           for index,seq in d \n",
    "    ]\n",
    "    \n",
    "    SeqIO.write(records2, 'Final_Merged_Certain' ,\"fasta\")\n",
    "    return \n",
    "\n",
    "def ExportMergeFinal(certaindf,ThreeDistMergingDf):\n",
    "    certaindf.to_csv('CertainReadings.csv',sep=',')\n",
    "    a = ThreeDistMergingDf.loc[ThreeDistMergingDf['C_Num']>=5,]\n",
    "    a.to_csv('CertainandUnCertainReadings.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2e1299-07dc-4338-9488-540bc375f7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119  rows reads with terminal gaps have been Masked.\n",
      "First three position of number Matrix: 3 1 2 Datatype: int64\n",
      "73  rows reads with terminal gaps have been Masked.\n",
      "314 of One Copy Readinds has not involved in merging, Merging happened up to 4 dist\n"
     ]
    }
   ],
   "source": [
    "    M,M_rows,M_cols,ids = CreateMatrix()\n",
    "    ReadingNameArray = CreateReadingNameArray(ids)\n",
    "    M,M_rows,M_cols,MissInfoRowCoords = RemoveTerminalGaps(M,M_rows, M_cols)\n",
    "    MissInfoRowCoords = np.sort(MissInfoRowCoords)\n",
    "    M_WithOutTerminalGapReadings = np.delete(M,(MissInfoRowCoords),axis=0)\n",
    "    ReMergeM = np.concatenate((M_WithOutTerminalGapReadings,M[MissInfoRowCoords]),axis=0)\n",
    "    ReorderReadNames = np.delete(ReadingNameArray,(MissInfoRowCoords),axis=0)\n",
    "    ReorderRN = np.concatenate((ReorderReadNames,ReadingNameArray[MissInfoRowCoords]),axis=0)   \n",
    "    ReadingD = CreateReadingNameDicts(ReorderRN)\n",
    "    M_rows,M_cols=ReMergeM.shape\n",
    "    ErrorMasking(ReMergeM,M_rows,M_cols,0.01)\n",
    "    M_Num = StringMatrixToNumber (ReMergeM)\n",
    "    counter,Mdis0,ReadingD = CalculatePairWiseDistance(M_Num,ReadingD)\n",
    "    M_Num = n_correction(Mdis0,M_Num)\n",
    "    array2 = reduction_Matrix(M_Num,counter)\n",
    "    reducedarray = array2[~np.all(array2 == 0, axis=1)]\n",
    "    array_rev = NumberMatrixToString (reducedarray) \n",
    "    ReadingD2 = ReducedReadingD(counter,ReadingD)\n",
    "    Reading_Pd= ReindexAfterReduction(ReadingD2)\n",
    "    Mdis = pairwise_distances(reducedarray,metric = CalculatePairWiseDistance_sk)\n",
    "    Export(Mdis)\n",
    "    array_rev = NumberMatrixToString (reducedarray)\n",
    "    array_rev = RemoveGap(array_rev)\n",
    "    Counter_Pd = GenerateCounterDataFrame(counter)\n",
    "    array_rev =TerminalGapAndNCorrection(array_rev,Counter_Pd)\n",
    "    seq_array = StringMatrixToSeq(array_rev)\n",
    "    FastaRecords(seq_array)\n",
    "    OneDist_pd =  CoordsOfOnedistInDistMatrix(Mdis)\n",
    "    droplistT1_array,Counter_Pd,Reading_Pd =FindUniqueOneDistReading(OneDist_pd,Counter_Pd,Reading_Pd) \n",
    "    OneCopyDf,OneCopyDfSeqID = OneCopyReadingList(Counter_Pd)\n",
    "    UnCertainlist,UnCertainID = DefineUnCertain(Counter_Pd,5,10)\n",
    "    Certainlist,CertainID = DefineCertain(Counter_Pd,10)\n",
    "    UnCertainlist,droplist_PartialUnCertain_array,Reading_Pd = Split_Merge_1copy_to_UnCertain(UnCertainlist,UnCertainID,OneCopyDfSeqID,Mdis,Counter_Pd,Reading_Pd,1)\n",
    "    Certainlist,droplist_PartialCertain_array,Reading_Pd= Split_Merge_1copy_to_certain(Certainlist,CertainID,OneCopyDfSeqID,Mdis,Counter_Pd,Reading_Pd,1)\n",
    "    OneCopyNotInvolved_df,OneCopyNotInvolved_array =  OneCopyreadingNotJoinedMerging(OneCopyDf,OneCopyDfSeqID,droplist_PartialUnCertain_array, droplist_PartialCertain_array)\n",
    "    arrayofInterest = np.concatenate([CertainID, UnCertainID, OneCopyNotInvolved_array])\n",
    "    autodistmerge(Mdis,arrayofInterest,OneCopyNotInvolved_df,OneCopyNotInvolved_array,Reading_Pd,ids,Counter_Pd)\n",
    "    Certain_UnCertain_Df  = Counter_Pd.loc[Counter_Pd['C_Num']>=5]\n",
    "    Certain_Df = Counter_Pd.loc[Counter_Pd['C_Num']>=10]\n",
    "    Certain_UnCertain = Certain_UnCertain_Df.index\n",
    "    Certain = Certain_Df.index\n",
    "    seq_df = pd.DataFrame(seq_array)\n",
    "    MergedSeqRecords = seq_df.iloc[Certain_UnCertain]\n",
    "    Certain_UnCertain = np.array(list(MergedSeqRecords[0]))\n",
    "    MergedSeqRecords2 = seq_df.iloc[Certain]\n",
    "    Certain = np.array(list(MergedSeqRecords2[0]))\n",
    "    IDlist,CU_count =CalChrCount(ids,Certain_UnCertain_Df)\n",
    "    CertainIDlist,C_count = CalChrCount(ids,Certain_Df)\n",
    "    CertainID_CNum  = CertainC_Numlist(Certain_Df,CertainIDlist)\n",
    "    C_count['SeqID'] = CertainID_CNum\n",
    "    C_count= C_count.set_index('SeqID',drop=True)\n",
    "    ExpChrCount(CU_count,C_count)\n",
    "    FastaRecords_Final_CandUnC(Certain_UnCertain,IDlist,18)\n",
    "    FastaRecords_Final_Certain(Certain,CertainID_CNum,18)\n",
    "    ExportMergeFinal(Certain_Df,Counter_Pd)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba834de-1422-4956-8829-ae747150d498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3457"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6cbccf-15d3-49f5-af81-4442d52bd5df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6604da-4547-4415-8d00-fa5b1539e19d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
